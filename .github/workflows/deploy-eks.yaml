name: Deploy EKS

on:
  workflow_dispatch:
    inputs:
      name:
        description: "EKS cluster name"
        required: true
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod
      destroy_on_failure:
        description: 'Destroy resources if deployment fails'
        required: false
        default: true
        type: boolean

env:
  TF_VERSION: 1.9.0
  KUBECTL_VERSION: 1.28.0

jobs:
  validate_and_plan:
    name: 🔍 Validate & Plan
    environment: ${{ github.event.inputs.environment }}
    runs-on: ubuntu-latest
    outputs:
      plan_exitcode: ${{ steps.plan.outputs.exitcode }}
      cluster_name: ${{ github.event.inputs.name }}
      
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ vars.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Validate inputs & AWS
        run: |
          # Validate cluster name
          if [[ ! "${{ github.event.inputs.name }}" =~ ^[a-zA-Z][a-zA-Z0-9-]*[a-zA-Z0-9]$ ]]; then
            echo "❌ Invalid cluster name"
            exit 1
          fi
          
          echo "🔗 Testing AWS connectivity..."
          aws sts get-caller-identity > /dev/null
          echo "✅ AWS connectivity verified"
          
          # Check if cluster exists
          if aws eks describe-cluster --name "${{ github.event.inputs.name }}" --region "${{ vars.AWS_REGION }}" 2>/dev/null; then
            echo "⚠️ Cluster already exists!"
          else
            echo "✅ Cluster name available"
          fi

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Init & Validate
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        run: |
          echo "🚀 Initializing Terraform..."
          terraform init \
            -backend-config="bucket=${{ vars.TF_STATE_BUCKET }}" \
            -backend-config="key=eks/${{ github.event.inputs.environment }}/${{ github.event.inputs.name }}/terraform.tfstate" \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -backend-config="dynamodb_table=${{ vars.TF_STATE_LOCK_TABLE }}" \
            -backend-config="encrypt=true"
          
          echo "✅ Validating Terraform..."
          terraform validate

      - name: Terraform Plan
        id: plan
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        env:
          TF_VAR_cluster_name: ${{ github.event.inputs.name }}
          TF_VAR_environment: ${{ github.event.inputs.environment }}
        run: |
          echo "📋 Creating Terraform plan..."
          terraform plan \
            -detailed-exitcode \
            -no-color \
            -out=tfplan
        

      - name: Terraform Init & Apply
        id: apply
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        env:
          TF_VAR_cluster_name: ${{ github.event.inputs.name }}
          TF_VAR_environment: ${{ github.event.inputs.environment }}
        run: |         
          echo "🚀 Deploying EKS cluster..."
          terraform apply -no-color tfplan
          echo "✅ Deployment completed"

      - name: Get Outputs
        id: output
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        run: |
          cluster_endpoint=$(terraform output -raw cluster_endpoint)
          cluster_name=$(terraform output -raw cluster_name)
          
          echo "cluster_endpoint=$cluster_endpoint" >> $GITHUB_OUTPUT
          echo "cluster_name=$cluster_name" >> $GITHUB_OUTPUT
          
          echo "📊 Cluster deployed: $cluster_name"

  verify:
    name: ✅ Verify & Connect
    needs: [validate_and_plan]
    if: needs.validate_and_plan.outputs.deployment_status == 'success'
    environment: ${{ github.event.inputs.environment }}
    runs-on: ubuntu-latest
    outputs:
      health_status: ${{ steps.health.outcome }}
      
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ vars.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Health Check
        id: health
        timeout-minutes: 15
        run: |
          echo "🔗 Configuring kubectl..."
          aws eks update-kubeconfig \
            --region ${{ vars.AWS_REGION }} \
            --name ${{ needs.validate_and_plan.outputs.cluster_name }}
          
          echo "⏳ Waiting for cluster readiness..."
          for i in {1..20}; do
            if kubectl cluster-info >/dev/null 2>&1; then
              echo "✅ Cluster responding"
              break
            fi
            echo "   Attempt $i/20..."
            sleep 30
          done
          
          echo "🏥 Health checks..."
          kubectl get nodes
          kubectl wait --for=condition=Ready nodes --all --timeout=600s
          kubectl get pods -n kube-system
          
          echo "✅ All health checks passed!"

      - name: Generate Connection Info
        run: |
          # Create connection script
          cat > connect.sh << 'EOF'
          #!/bin/bash
          echo "🔗 Connecting to EKS cluster..."
          aws eks update-kubeconfig \
            --region ${{ vars.AWS_REGION }} \
            --name ${{ needs.deploy.outputs.cluster_name }}
          echo "✅ Connected! Try: kubectl get nodes"
          EOF
          chmod +x connect.sh

      - name: Upload Connection Script
        uses: actions/upload-artifact@v4
        with:
          name: connect-${{ github.event.inputs.name }}
          path: ./connect.sh
          retention-days: 7

      - name: Success Summary
        run: |
          echo "🎉 EKS Cluster Ready!"
          echo "====================="
          echo "Name: ${{ needs.deploy.outputs.cluster_name }}"
          echo "Region: ${{ vars.AWS_REGION }}"
          echo ""
          echo "🔗 Connect with:"
          echo "aws eks update-kubeconfig --region ${{ vars.AWS_REGION }} --name ${{ needs.deploy.outputs.cluster_name }}"

  cleanup:
    name: 🧹 Cleanup on Failure
    needs: [validate_and_plan, verify]
    if: |
      always() &&
      github.event.inputs.destroy_on_failure == 'true' &&
      (
        needs.validate_and_plan.result == 'failure' ||
        needs.verify.result == 'failure' ||
        needs.validate_and_plan.result == 'cancelled' ||
        needs.verify.result == 'cancelled'
      )
    environment: ${{ github.event.inputs.environment }}
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ vars.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Setup Terraform & Destroy
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Cleanup Failed Deployment
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        env:
          TF_VAR_cluster_name: ${{ github.event.inputs.name }}
          TF_VAR_environment: ${{ github.event.inputs.environment }}
        run: |
          terraform init \
            -backend-config="bucket=${{ vars.TF_STATE_BUCKET }}" \
            -backend-config="key=eks/${{ github.event.inputs.environment }}/${{ github.event.inputs.name }}/terraform.tfstate" \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -backend-config="dynamodb_table=${{ vars.TF_STATE_LOCK_TABLE }}" \
            -backend-config="encrypt=true"
          
          echo "🧹 Destroying failed deployment..."
          terraform destroy -var-file="terraform.tfvars" -auto-approve
          echo "✅ Cleanup completed"