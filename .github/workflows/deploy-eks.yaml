name: Deploy EKS

on:
  workflow_dispatch:
    inputs:
      name:
        description: "EKS cluster name"
        required: true
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod
      destroy_on_failure:
        description: 'Destroy resources if deployment fails'
        required: false
        default: true
        type: boolean

env:
  TF_VERSION: 1.9.0
  KUBECTL_VERSION: 1.28.0

jobs:
  deploy:
    name: 🚀 Deploy EKS Cluster
    environment: ${{ github.event.inputs.environment }}
    runs-on: ubuntu-latest
    outputs:
      cluster_name: ${{ github.event.inputs.name }}
      cluster_endpoint: ${{ steps.output.outputs.cluster_endpoint }}
      deployment_status: ${{ steps.deployment_status.outputs.status }}
      
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ vars.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Validate inputs & AWS
        run: |
          # Validate cluster name
          if [[ ! "${{ github.event.inputs.name }}" =~ ^[a-zA-Z][a-zA-Z0-9-]*[a-zA-Z0-9]$ ]]; then
            echo "❌ Invalid cluster name"
            exit 1
          fi
          
          echo "🔗 Testing AWS connectivity..."
          aws sts get-caller-identity > /dev/null
          echo "✅ AWS connectivity verified"
          
          # Check if cluster exists
          if aws eks describe-cluster --name "${{ github.event.inputs.name }}" --region "${{ vars.AWS_REGION }}" 2>/dev/null; then
            echo "⚠️ Cluster already exists!"
          else
            echo "✅ Cluster name available"
          fi

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Init & Validate
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        run: |
          echo "🚀 Initializing Terraform..."
          terraform init \
            -backend-config="bucket=${{ vars.TF_STATE_BUCKET }}" \
            -backend-config="key=eks/${{ github.event.inputs.environment }}/${{ github.event.inputs.name }}/terraform.tfstate" \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -backend-config="dynamodb_table=${{ vars.TF_STATE_LOCK_TABLE }}" \
            -backend-config="encrypt=true"
          
          echo "✅ Validating Terraform..."
          terraform validate

      - name: Terraform Plan
        id: plan
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        env:
          TF_VAR_cluster_name: ${{ github.event.inputs.name }}
          TF_VAR_environment: ${{ github.event.inputs.environment }}
        run: |
          echo "📋 Creating Terraform plan..."
          terraform plan \
            -detailed-exitcode \
            -no-color \
            -out=tfplan

      - name: Terraform Apply
        id: apply
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        env:
          TF_VAR_cluster_name: ${{ github.event.inputs.name }}
          TF_VAR_environment: ${{ github.event.inputs.environment }}
        run: |         
          echo "🚀 Deploying EKS cluster..."
          terraform apply -no-color tfplan
          echo "✅ Deployment completed"

      - name: Get Outputs
        id: output
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        run: |
          cluster_endpoint=$(terraform output -raw cluster_endpoint)
          cluster_name=$(terraform output -raw cluster_name)
          
          echo "cluster_endpoint=$cluster_endpoint" >> $GITHUB_OUTPUT
          echo "cluster_name=$cluster_name" >> $GITHUB_OUTPUT
          
          echo "📊 Cluster deployed: $cluster_name"

      - name: Set deployment status
        id: deployment_status
        if: always()
        run: |
          if [[ "${{ steps.apply.outcome }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "status=failed" >> $GITHUB_OUTPUT
          fi

  verify:
    name: ✅ Verify & Connect
    needs: [deploy]
    if: needs.deploy.outputs.deployment_status == 'success'
    environment: ${{ github.event.inputs.environment }}
    runs-on: ubuntu-latest
    outputs:
      health_status: ${{ steps.health.outcome }}
      
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ vars.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Health Check
        id: health
        timeout-minutes: 15
        run: |
          echo "🔗 Configuring kubectl..."
          aws eks update-kubeconfig \
            --region ${{ vars.AWS_REGION }} \
            --name ${{ needs.deploy.outputs.cluster_name }}
          
          echo "⏳ Waiting for cluster readiness..."
          for i in {1..20}; do
            if kubectl cluster-info >/dev/null 2>&1; then
              echo "✅ Cluster responding"
              break
            fi
            echo "   Attempt $i/20..."
            sleep 30
          done
          
          echo "🏥 Health checks..."
          kubectl get nodes
          kubectl wait --for=condition=Ready nodes --all --timeout=600s
          kubectl get pods -n kube-system
          
          echo "✅ All health checks passed!"

      - name: Generate Connection Info
        run: |
          # Create connection script
          cat > connect.sh << 'EOF'
          #!/bin/bash
          echo "🔗 Connecting to EKS cluster..."
          aws eks update-kubeconfig \
            --region ${{ vars.AWS_REGION }} \
            --name ${{ needs.deploy.outputs.cluster_name }}
          echo "✅ Connected! Try: kubectl get nodes"
          EOF
          chmod +x connect.sh

      - name: Upload Connection Script
        uses: actions/upload-artifact@v4
        with:
          name: connect-${{ github.event.inputs.name }}
          path: ./connect.sh
          retention-days: 7

      - name: Success Summary
        run: |
          echo "🎉 EKS Cluster Ready!"
          echo "====================="
          echo "Name: ${{ needs.deploy.outputs.cluster_name }}"
          echo "Region: ${{ vars.AWS_REGION }}"
          echo ""
          echo "🔗 Connect with:"
          echo "aws eks update-kubeconfig --region ${{ vars.AWS_REGION }} --name ${{ needs.deploy.outputs.cluster_name }}"

  cleanup:
    name: 🧹 Cleanup on Failure
    needs: [deploy, verify]
    if: |
      always() &&
      github.event.inputs.destroy_on_failure == 'true' &&
      (
        needs.deploy.result == 'failure' ||
        needs.deploy.result == 'cancelled' ||
        needs.verify.result == 'failure' ||
        needs.verify.result == 'cancelled' ||
        cancelled()
      )
    environment: ${{ github.event.inputs.environment }}
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ vars.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Check if resources exist
        id: check_resources
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        env:
          TF_VAR_cluster_name: ${{ github.event.inputs.name }}
          TF_VAR_environment: ${{ github.event.inputs.environment }}
        run: |
          # Initialize Terraform
          terraform init \
            -backend-config="bucket=${{ vars.TF_STATE_BUCKET }}" \
            -backend-config="key=eks/${{ github.event.inputs.environment }}/${{ github.event.inputs.name }}/terraform.tfstate" \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -backend-config="dynamodb_table=${{ vars.TF_STATE_LOCK_TABLE }}" \
            -backend-config="encrypt=true"
          
          # Check if state has resources
          if terraform state list 2>/dev/null | grep -q .; then
            echo "resources_exist=true" >> $GITHUB_OUTPUT
            echo "📋 Found resources in state to clean up"
          else
            echo "resources_exist=false" >> $GITHUB_OUTPUT
            echo "ℹ️ No resources found in state"
          fi

      - name: Force Cleanup Failed/Cancelled Deployment
        if: steps.check_resources.outputs.resources_exist == 'true'
        working-directory: ./cluster/terraform/environments/${{ github.event.inputs.environment }}
        env:
          TF_VAR_cluster_name: ${{ github.event.inputs.name }}
          TF_VAR_environment: ${{ github.event.inputs.environment }}
        run: |
          echo "🧹 Starting cleanup of failed/cancelled deployment..."
          echo "⚠️ This may take 10-15 minutes..."
          
          # First try normal destroy
          if terraform destroy -auto-approve -no-color 2>&1 | tee destroy.log; then
            echo "✅ Normal cleanup completed successfully"
          else
            echo "⚠️ Normal destroy failed, attempting force cleanup..."
            
            # If normal destroy fails, try removing finalizers and force destroy
            if aws eks describe-cluster --name "${{ github.event.inputs.name }}" --region "${{ vars.AWS_REGION }}" 2>/dev/null; then
              echo "🔧 Removing Kubernetes finalizers..."
              aws eks update-kubeconfig --region ${{ vars.AWS_REGION }} --name ${{ github.event.inputs.name }} || true
              
              # Remove finalizers from persistent volumes
              kubectl get pv -o name 2>/dev/null | xargs -I {} kubectl patch {} -p '{"metadata":{"finalizers":null}}' --type=merge || true
              
              # Remove finalizers from persistent volume claims
              kubectl get pvc --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name --no-headers 2>/dev/null | \
                while read ns name; do
                  kubectl patch pvc $name -n $ns -p '{"metadata":{"finalizers":null}}' --type=merge || true
                done
              
              # Delete load balancers
              kubectl get svc --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,TYPE:.spec.type --no-headers 2>/dev/null | \
                grep LoadBalancer | while read ns name type; do
                  kubectl delete svc $name -n $ns --force --grace-period=0 || true
                done
            fi
            
            # Try destroy again with refresh
            echo "🔄 Attempting destroy with state refresh..."
            terraform destroy -auto-approve -refresh=true -no-color || true
            
            # Final attempt - remove from state and destroy
            echo "🔨 Final cleanup attempt..."
            terraform state list | while read resource; do
              echo "Removing $resource from state..."
              terraform state rm "$resource" || true
            done
            
            # One more destroy to catch any remaining resources
            terraform destroy -auto-approve -no-color || true
          fi
          
          echo "✅ Cleanup process completed"

      - name: Cleanup Summary
        if: always()
        run: |
          echo "🧹 Cleanup Job Summary"
          echo "===================="
          echo "Workflow Status:"
          echo "  - Deploy: ${{ needs.deploy.result }}"
          echo "  - Verify: ${{ needs.verify.result }}"
          echo "  - Cleanup triggered: YES"
          echo "  - Resources found: ${{ steps.check_resources.outputs.resources_exist }}"
          echo ""
          if [[ "${{ steps.check_resources.outputs.resources_exist }}" == "true" ]]; then
            echo "✅ Cleanup was attempted for failed/cancelled deployment"
          else
            echo "ℹ️ No resources to clean up"
          fi